<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="description" content="先前我们在最小二乘拟合那一节中了解到如何使用最小二乘的方法拟合多项式方程，那么今天我们再来学习一种「拟合」方程的方法——逻辑回归。">
<meta name="keywords" content="python,神经网络,人工智能,pytorch,tensor,ai,mnist,softmax,cnn">
<meta name="author" content="bxtkezhan@kk">
<meta name="generator" content="Hugo 0.121.1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type="text/css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="bxtkezhan@kk">
<title>人工神经网络・逻辑回归 - bxtkezhan@kk</title>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7VMHQ286BK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-7VMHQ286BK');
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<script>MathJax = {tex: {inlineMath: [['$', '$']]}, svg: {fontCache: 'global'}};</script>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>
<script src="/js/player.js"></script>
</head>
<body>

<header>
  <div class="container clearfix">
    <a class="path" href="http://bxtkezhan.github.io/">[bxtkezhan@kk]</a>
    <span class="caret"># _</span>
    <div class="right">
      
      
        <a class="path" style="color: var(--base0e)"href="/widle/">widle</a>
      
        <a class="path" style="color: var(--base0e)"href="/strut/">strut</a>
      
    </div>
  </div>
</header>

<div class="container">


<main role="main" class="article">
  
<article class="single" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="meta">

    <span class="key">published on</span>
    <span class="val"><time itemprop="datePublished" datetime="2022-09-26">September 26, 2022</time></span>


    <span class="key">in</span>
    <span class="val">

        <a href="/categories/python">Python</a>

    </span>


  </div>
  <h1 class="headline" itemprop="headline">人工神经网络・逻辑回归</h1>
  <section class="body" itemprop="articleBody">
    <p><a href="/post/tutorial_001/">返回教程主页</a></p>
<p><a href="/post/062-artificial-neural-network-env/">上篇 人工神经网络・操作环境准备</a></p>
<p>先前我们在最小二乘拟合那一节中了解到如何使用最小二乘的方法拟合多项式方程，那么今天我们再来学习一种「拟合」方程的方法——逻辑回归。</p>
<p>我们要处理的问题是这样的:</p>
<p>需要一个方程能够检测两位的二进制数字「包括 00 01 10 11」是否为奇数，当输入为奇数时方程输出1，输入为偶数时方程输出0。</p>
<p>上述方程接收输入并输出类似逻辑电路中高低电平信号的0，1值，很容易让人联想到使用逻辑回归方法去求解。因此，接下来我们就尝试使用逻辑回归进行求解。</p>
<h3 id="定义函数">定义函数</h3>
<p>我们的方程表达式如下:</p>
<p>$$
y = \sigma(XW + b)
$$</p>
<p>其中X为方程输入，y为方程输出。W与b为方程的参数，具体数值需要经过拟合求得。</p>
<p>方程的输入X为二进制数转换成的矩阵，二进制数与矩阵的对应关系如下:</p>
<p>$$
\begin{align}
00 \rightarrow \begin{bmatrix} 0 &amp; 0 \end{bmatrix} \\
01 \rightarrow \begin{bmatrix} 0 &amp; 1 \end{bmatrix} \\
10 \rightarrow \begin{bmatrix} 1 &amp; 0 \end{bmatrix} \\
11 \rightarrow \begin{bmatrix} 1 &amp; 1 \end{bmatrix} \\
\end{align}
$$</p>
<p>参数W是个2x1的矩阵: $W = \begin{bmatrix}w_1 \\ w_2 \end{bmatrix}$；参数b则为一个标量。</p>
<p>函数$\sigma(x)$为著名的sigmoid函数:</p>
<p>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</p>
<p><img src="/img/sigmoid.png" alt="sigmoid.png"></p>
<p>从函数图可以看出该函数的输出会落在0与1之间，并且在输入大于0的时候函数输出会快速趋近1，输入小于0的时候函数输出则会快速趋近0。</p>
<h3 id="优化学习">优化学习</h3>
<p>为了求解具体的参数数值，我们可以先预设参数，然后对参数进行优化更新，使其变成符合我们期望的参数。</p>
<p>在对参数进行优化前，我们还得设计一个方案去评估参数的效果，如果不知道当前参数的效果我们是无法顺利地对参数进行优化的。</p>
<p>一般来说逻辑回归可以使用 <strong>梯度下降算法</strong> 进行参数优化，而该算法应用在逻辑回归时也有常用的参数评估函数「或称 <strong>损失函数</strong>」:</p>
<p>$$
loss = \sum_{i=1}^n\frac{1}{2}(t_i - p_i)^2
$$</p>
<p>在上述方程中$t_i$为第i个真实输出「或者说是预期中的函数输出」，而$p_i$则是与之对应的当前函数给出的输出。</p>
<p>假设X为输入，期望中的函数为$f_t$，当前函数为$f_p$，那么t与p的对应关系大致如下:</p>
<p>$$
\begin{pmatrix}
t_1 \leftrightarrow p_1 \\
t_2 \leftrightarrow p_2 \\
t_3 \leftrightarrow p_3 \\
t_4 \leftrightarrow p_4 \\
\end{pmatrix}
\left\{
\begin{align}
\begin{bmatrix} t_1 \\ t_2 \\ t_3 \\ t_4 \end{bmatrix} = T &amp; = f_t(X) \\
\begin{bmatrix} p_1 \\ p_2 \\ p_3 \\ p_4 \end{bmatrix} = P &amp; = f_p(X) \\
\end{align}
\right.
\quad , \quad
X = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp; 0 \\ 1 &amp; 1 \end{bmatrix}
$$</p>
<p>在计算完损失函数值后，我们就对损失函数求参数$\{W,b\}$的梯度$\{\Delta W,\Delta b\}$，然后用当前的参数分别减去各自的梯度用以得到更新后的参数:</p>
<p>$$
W_t = W_{t-1} - \lambda \Delta W \ ; \
b_t = b_{t-1} - \lambda \Delta b
$$</p>
<p>通常在使用梯度进行参数更新的时候我们不希望使用全部的梯度，这样会使得数值跨度太大，因此我们通常会将梯度乘上一个小于1的正数$\lambda$，很多地方称$\lambda$为学习率「learning rate」。</p>
<p>了解了这些后我们开始编写代码。</p>
<h3 id="准备输入输出">准备输入输出</h3>
<p>在确保安装了pytorch后我们在代码中引入它:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span></code></pre></div><p><code>torch.tensor</code>类似numpy中的<code>numpy.array</code>，我们可以使用它创建张量「包括标量，向量，矩阵……」。在这里，我们使用张量存储方程的输入X与输出T。X为4x2的矩阵而T为4x1的矩阵，它们的每一行按顺序互相对应:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>T <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span></code></pre></div><p>这里需要注意的是我们大多数情况会使用32位的浮点数值<code>torch.float32</code>，这样做既能满足精度的需要又避免过多的运算。</p>
<h3 id="准备需要拟合的参数">准备需要拟合的参数</h3>
<p>公式 $y = \sigma(XW + b)$ 中的W「常常称作权重Weights」与b「常常称作偏置bias」是我们需要拟合的参数:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, ), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>我们将权重初始化为元素都是1的矩阵，而偏置则初始化为0。另外，由于我们需要求解权重和偏置的梯度，所以我们需要设置参数<code>requires_grad=True</code>。</p>
<h3 id="正向传播">正向传播</h3>
<p>我们在一个for循环中编写正向传播的代码:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># forward</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W
</span></span><span style="display:flex;"><span>    H <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(H)
</span></span></code></pre></div><p>代码是根据公式 $y = \sigma(XW + b)$ 编写的，经由sigmoid函数处理后的输出P就对应公式中的输出y。</p>
<h3 id="计算损失函数">计算损失函数</h3>
<p>我们根据如下公式编写损失函数:</p>
<p>$$
loss = \sum_{i=1}^n\frac{1}{2}(t_i - p_i)^2
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> (T <span style="color:#f92672">-</span> P)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;step:&#39;</span>, step, <span style="color:#e6db74">&#39;loss =&#39;</span>, loss<span style="color:#f92672">.</span>item())
</span></span></code></pre></div><h3 id="反向传播">反向传播</h3>
<p>调用<code>loss.backward</code>方法，对支持梯度求解的张量对象<code>loss</code>进行梯度求解:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># backward</span>
</span></span><span style="display:flex;"><span>loss<span style="color:#f92672">.</span>backward()
</span></span></code></pre></div><p>在执行完反向传播操作后，我们可以得到权重的梯度<code>W.grad</code>以及偏置的梯度<code>b.grad</code>。</p>
<h3 id="更新梯度">更新梯度</h3>
<p>由于我们使用的是梯度下降优化方案所以在更新权重和偏置的时候需要减去梯度。但是如果直接减去梯度则可能步长过大，所以通常会将梯度乘上一个小于1大于0的值<code>lr</code>，然后用当前权重减去它，正如下方那样:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>	W <span style="color:#f92672">-=</span> W<span style="color:#f92672">.</span>grad <span style="color:#f92672">*</span> lr
</span></span><span style="display:flex;"><span>	b <span style="color:#f92672">-=</span> b<span style="color:#f92672">.</span>grad <span style="color:#f92672">*</span> lr
</span></span><span style="display:flex;"><span>	W<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>	b<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span></code></pre></div><p>在pytorch中，更新带有梯度的参数时，我们需要在不计算梯度的模式下<code>with torch.no_grad():</code>进行；而在更新梯度后，我们需要使用<code>tensor.grad.data.zero_()</code>方法清零当前的梯度。</p>
<h3 id="查看拟合后的参数">查看拟合后的参数</h3>
<p>完成100轮上述的权重与偏置的更新后我们打印输出一下权重和偏置:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;params weight:&#39;</span>)
</span></span><span style="display:flex;"><span>print(W<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;params bias:&#39;</span>)
</span></span><span style="display:flex;"><span>print(b<span style="color:#f92672">.</span>detach())
</span></span></code></pre></div><p><code>tensor.detach()</code>方法用于将普通tensor从带梯度的tensor中分离，此处使用仅为了打印输出的美观。</p>
<h3 id="测试拟合效果">测试拟合效果</h3>
<p>我们在不计算梯度的模式下测试一下拟合后的方程。在这里我们使用pytorch张量对象的列表索引<code>X[[0, 2, 1, 3]]</code>改变了一下矩阵X的行顺序，那么对应的输出P则应该趋近于$\begin{bmatrix}0 \\ 0 \\ 1 \\ 1\end{bmatrix}$:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X[[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>]] <span style="color:#f92672">@</span> W
</span></span><span style="display:flex;"><span>    H <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(H)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;predict:&#39;</span>)
</span></span><span style="display:flex;"><span>    print(P)
</span></span></code></pre></div><p>运行后输出:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>predict:
</span></span><span style="display:flex;"><span>tensor([[<span style="color:#ae81ff">0.2789</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.3257</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.7348</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.7759</span>]])
</span></span></code></pre></div><p>我们发现输出的头两行元素小于0.5趋近0，后两行元素大于0.5趋近1，基本符合我们的预期，这时我们可以使用<code>torch.round</code>进行四舍五入将其严格转换为$\begin{bmatrix}0 \\ 0 \\ 1 \\ 1\end{bmatrix}$的结构:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(torch<span style="color:#f92672">.</span>round(P))
</span></span></code></pre></div><p>运行后输出:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([[<span style="color:#ae81ff">0.</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0.</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">1.</span>]])
</span></span></code></pre></div><h3 id="完整代码">完整代码</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>T <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>W <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, ), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># forward</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W
</span></span><span style="display:flex;"><span>    H <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(H)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> (T <span style="color:#f92672">-</span> P)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;step:&#39;</span>, step, <span style="color:#e6db74">&#39;loss =&#39;</span>, loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># backward</span>
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        W <span style="color:#f92672">-=</span> W<span style="color:#f92672">.</span>grad <span style="color:#f92672">*</span> lr
</span></span><span style="display:flex;"><span>        b <span style="color:#f92672">-=</span> b<span style="color:#f92672">.</span>grad <span style="color:#f92672">*</span> lr
</span></span><span style="display:flex;"><span>        W<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>        b<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;params weight:&#39;</span>)
</span></span><span style="display:flex;"><span>print(W<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;params bias:&#39;</span>)
</span></span><span style="display:flex;"><span>print(b<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X[[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>]] <span style="color:#f92672">@</span> W
</span></span><span style="display:flex;"><span>    H <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(H)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;predict:&#39;</span>)
</span></span><span style="display:flex;"><span>    print(P)
</span></span><span style="display:flex;"><span>print(torch<span style="color:#f92672">.</span>round(P))
</span></span></code></pre></div><p><a href="/post/064-softmax-clf/">下篇 人工神经网络・Softmax多分类</a></p>

  </section>
</article>

</main>

</div>

<footer>
  <div class="container">
    友情分享:
    
    <span><a href="https://wd.bible/">微读圣经</a></span>
    
    <span><a href="https://cnbible.com/">网上圣经</a></span>
    
  </div>
  <div class="container">
    <span class="copyright">&copy; 2024 bxtkezhan@kk - <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></span>
  </div>
</footer>

</body>
</html>

