<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="description" content="逻辑回归常用于处理二分类问题的，虽然也可以通过〚联合〛的方式处理多分类问题「有超过两个以上的类别」但这样比较麻烦。在处理多分类问题的时候我们可以使用softmax替代逻辑回归里的sigmoid激活函数，这样可以实现多分类操作。">
<meta name="keywords" content="python,神经网络,人工智能,pytorch,tensor,ai,mnist,softmax,cnn">
<meta name="author" content="bxtkezhan@kk">
<meta name="generator" content="Hugo 0.92.2" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type="text/css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="bxtkezhan@kk">
<title>人工神经网络・Softmax多分类 - bxtkezhan@kk</title>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7VMHQ286BK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-7VMHQ286BK');
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<script>MathJax = {tex: {inlineMath: [['$', '$']]}, svg: {fontCache: 'global'}};</script>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>
<script src="/js/player.js"></script>
</head>
<body>

<header>
  <div class="container clearfix">
    <a class="path" href="http://bxtkezhan.xyz/">[bxtkezhan@kk]</a>
    <span class="caret"># _</span>
    <div class="right">
      
      
        <a class="path" style="color: var(--base0e)"href="/widle/">widle</a>
      
        <a class="path" style="color: var(--base0e)"href="/strut/">strut</a>
      
    </div>
  </div>
</header>

<div class="container">


<main role="main" class="article">
  
<article class="single" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="meta">

    <span class="key">published on</span>
    <span class="val"><time itemprop="datePublished" datetime="2022-09-29">September 29, 2022</time></span>


    <span class="key">in</span>
    <span class="val">

        <a href="/categories/python">Python</a>

    </span>


  </div>
  <h1 class="headline" itemprop="headline">人工神经网络・Softmax多分类</h1>
  <section class="body" itemprop="articleBody">
    <p><a href="/post/tutorial_001/">返回教程主页</a></p>
<p><a href="/post/063-logistic-regression/">上篇 人工神经网络・逻辑回归</a></p>
<p>逻辑回归常用于处理二分类问题的，虽然也可以通过〚联合〛的方式处理多分类问题「有超过两个以上的类别」但这样比较麻烦。在处理多分类问题的时候我们可以使用softmax替代逻辑回归里的sigmoid激活函数，这样可以实现多分类操作。</p>
<h3 id="softmax函数">Softmax函数</h3>
<p>我们简单介绍一下Softmax函数，它的数学公式如下:</p>
<p>$$
\begin{align}
y_i = \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}
\end{align}
$$</p>
<p>当我们有一个数据 $x = \begin{bmatrix}x_1 &amp; x_2 &amp; x_3\end{bmatrix}$ 输入到softmax，它会输出的 $y$ 值将如下:</p>
<p>$$
y = \begin{bmatrix}y_1 &amp; y_2 &amp; y_3\end{bmatrix}
\ \Leftrightarrow\
\left\{
\begin{align}
y_1 = \frac{e^{x_1}}{\sum_{j=1}^{n}e^{x_j}} \\
y_2 = \frac{e^{x_3}}{\sum_{j=1}^{n}e^{x_j}} \\
y_3 = \frac{e^{x_2}}{\sum_{j=1}^{n}e^{x_j}} \\
\end{align}
\right.
$$</p>
<p>我们用一个具体的数值来演示一下，假设输入数据如下:</p>
<p>$$
x = \begin{bmatrix}0 &amp; 1 &amp; 2\end{bmatrix}
$$</p>
<p>于是计算过程如下:</p>
<p>$$
\begin{align}
\sum_{j=1}^{3}e^{x_j} &amp;= e^0 + e^1 + e^2 \\
&amp;= 1 + 2.71828183 + 7.3890561 \\
&amp;= 11.107337927389695 \\
y_1 &amp;= \frac{e^{x_1}}{\sum_{j=1}^{3}e^{x_j}} = 0.09003057 \\
y_2 &amp;= \frac{e^{x_2}}{\sum_{j=1}^{3}e^{x_j}} = 0.24472847 \\
y_3 &amp;= \frac{e^{x_3}}{\sum_{j=1}^{3}e^{x_j}} = 0.66524096 \\
\end{align}
$$</p>
<p>最终输出y为:</p>
<p>$$
y = \begin{bmatrix}0.09003057 &amp; 0.24472847 &amp; 0.66524096\end{bmatrix}
$$</p>
<p>y的每一个元素的值会落在0到1之间，并且将y的每一个元素值相加会得到数值1，因此softmax的输出常常用于代表各个类别的概率，例如当第一个元素的值为0.09003057时代表有9.003057%的可能性为第一个类别；第二个元素的值为0.24472847代表有24.472847%的可能性为第二个类别……以此类推。</p>
<h3 id="引入所需的模块">引入所需的模块</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">import</span> torch
torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">7</span>)

<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</code></pre></div><p><code>torch.manual_seed(7)</code>将pytorch的随机生成器的种子固定在7，这样可以让每一次运行的结果相同。</p>
<p>由于我们需要绘制图像，所以除了引入pytorch我们还引入了matplotlib；<code>torch.nn.functional</code>包含pytorch中大量的神经网络相关的函数，我们将用到它。</p>
<h3 id="构建输入输出">构建输入输出</h3>
<p>我们随机产生一个10x5的矩阵X作为输入，它的每一行为一个数据点，共计10个数据点。</p>
<p>除此之外，我们还创建了一个索引<code>index</code>，它的生成方法是: 将矩阵X与另一个5x3的随机矩阵相乘得到一个10x3的新矩阵，然后取这个新矩阵中每一行的最大值元素的下标。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
index <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(X <span style="color:#f92672">@</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
print(<span style="color:#e6db74">&#39;index:&#39;</span>)
print(index)
</code></pre></div><p>我们还需要使用one-hot函数将索引<code>index</code>进行转换，让它变成一个元素值为0或1的10x3矩阵，该矩阵的每一行只有一个1，并且第n行中值为1的元素的下标等于索引<code>index</code>第n行的元素值，即有: <code>T[n, index[n]]</code>的值为1:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">T <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>one_hot(index, <span style="color:#ae81ff">3</span>)<span style="color:#f92672">.</span>type(torch<span style="color:#f92672">.</span>float32)
print(<span style="color:#e6db74">&#39;one hot:&#39;</span>)
print(T)
</code></pre></div><h3 id="设置需要拟合的参数">设置需要拟合的参数</h3>
<p>由于我们的输出是个nx3的矩阵所以权重W的维度是5x3，偏置b的维度是3:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">W <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">3</span>, ), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><h3 id="正向传播">正向传播</h3>
<p>在一个for循环中执行正向传播代码:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
losses <span style="color:#f92672">=</span> []
print(<span style="color:#e6db74">&#39;training...&#39;</span>)
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(steps):
    A <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W <span style="color:#f92672">+</span> b
    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(A, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(P, T)

    losses<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
    i <span style="color:#f92672">=</span> step <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;step: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>steps<span style="color:#e6db74">}</span><span style="color:#e6db74">, loss=</span><span style="color:#e6db74">{</span>losses[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</code></pre></div><p><code>torch.softmax(A, dim=1)</code>调用softmax激活函数需要指定维度<code>dim=1</code>这是因为矩阵A的每一行对应一个数据点，我们需要单独给每一行进行softmax。由于使用的是softmax激活函数，因此我们的损失函数需要采用交叉熵损失函数即<code>F.cross_entropy</code>，该函数的第一个参数为softmax的输出，第二个参数为正确的分类标识。</p>
<p>我们使用列表<code>losses</code>存储每一轮的损失值<code>loss.item()</code>，并每10个循环打印输出一次。</p>
<h3 id="反向传播">反向传播</h3>
<p>调用<code>loss.backward()</code>进行反向传播用以求解W与b的梯度，然后在不计算梯度的模式下进行更新:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loss<span style="color:#f92672">.</span>backward()
lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
	W <span style="color:#f92672">-=</span> lr <span style="color:#f92672">*</span> W<span style="color:#f92672">.</span>grad
	b <span style="color:#f92672">-=</span> lr <span style="color:#f92672">*</span> b<span style="color:#f92672">.</span>grad
	W<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
	b<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</code></pre></div><h3 id="查看拟合效果">查看拟合效果</h3>
<p>打印输出矩阵P，矩阵P的每一行的数值处于0到1之间，代表改行所对应的数据点在某个类别上的概率。例如: P的第0行的0个元素为0.2115则表示第0个数据点属于第0个类别的概率为21.15%，我们可以调用<code>torch.argmax(P, dim=1)</code>将矩阵P转换成类似<code>index</code>的索引向量:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(<span style="color:#e6db74">&#39;probabilities:&#39;</span>)
print(P<span style="color:#f92672">.</span>detach())
print(<span style="color:#e6db74">&#39;true:&#39;</span>, index)
<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
    print(<span style="color:#e6db74">&#39;pred:&#39;</span>, torch<span style="color:#f92672">.</span>argmax(P, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
</code></pre></div><p>运行后输出:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">probabilities:
tensor([[<span style="color:#ae81ff">0.2115</span>, <span style="color:#ae81ff">0.7491</span>, <span style="color:#ae81ff">0.0394</span>],
        [<span style="color:#ae81ff">0.1779</span>, <span style="color:#ae81ff">0.0743</span>, <span style="color:#ae81ff">0.7478</span>],
        [<span style="color:#ae81ff">0.2192</span>, <span style="color:#ae81ff">0.4239</span>, <span style="color:#ae81ff">0.3569</span>],
        [<span style="color:#ae81ff">0.0591</span>, <span style="color:#ae81ff">0.0165</span>, <span style="color:#ae81ff">0.9244</span>],
        [<span style="color:#ae81ff">0.0901</span>, <span style="color:#ae81ff">0.7963</span>, <span style="color:#ae81ff">0.1136</span>],
        [<span style="color:#ae81ff">0.9610</span>, <span style="color:#ae81ff">0.0273</span>, <span style="color:#ae81ff">0.0117</span>],
        [<span style="color:#ae81ff">0.7126</span>, <span style="color:#ae81ff">0.1436</span>, <span style="color:#ae81ff">0.1438</span>],
        [<span style="color:#ae81ff">0.9565</span>, <span style="color:#ae81ff">0.0049</span>, <span style="color:#ae81ff">0.0386</span>],
        [<span style="color:#ae81ff">0.9441</span>, <span style="color:#ae81ff">0.0457</span>, <span style="color:#ae81ff">0.0102</span>],
        [<span style="color:#ae81ff">0.1677</span>, <span style="color:#ae81ff">0.4086</span>, <span style="color:#ae81ff">0.4237</span>]])
true: tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>])
pred: tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>])
</code></pre></div><h3 id="绘制拟合过程中的损失率">绘制拟合过程中的损失率</h3>
<p>最后我们还可以可视化一下拟合过程中损失率的变化:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Step&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;CrossEntropyLoss&#39;</span>)
plt<span style="color:#f92672">.</span>plot(range(len(losses)), losses, <span style="color:#e6db74">&#39;-&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p><img src="/img/softmax_fit_loss.png" alt="softmax_fit_loss.png"></p>

  </section>
</article>

</main>

</div>

<footer>
  <div class="container">
    友情分享:
    
    <span><a href="https://wd.bible/">微读圣经</a></span>
    
    <span><a href="https://cnbible.com/">网上圣经</a></span>
    
  </div>
  <div class="container">
    <span class="copyright">&copy; 2022 bxtkezhan@kk - <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></span>
  </div>
</footer>

</body>
</html>

